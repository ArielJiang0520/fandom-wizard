{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "nominated-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm, trange\n",
    "import time\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "japanese-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_by_id(id_):\n",
    "    url = 'https://archiveofourown.org/works/{}?view_full_work=true'\n",
    "    headers = {'user-agent': 'bot (sj784@cornell.edu)'}\n",
    "    \n",
    "    r = requests.get(url.format(id_), headers=headers)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    \n",
    "    assert soup != None\n",
    "        \n",
    "    stats = soup.find('dd', class_='stats')\n",
    "    \n",
    "    assert stats != None\n",
    "    \n",
    "    published_date = stats.find('dl', class_='stats').find('dd', class_='published').get_text() \\\n",
    "        if stats.find('dd', class_='published') else ''\n",
    "    \n",
    "    output = ''\n",
    "    if soup.find_all('div', class_=\"userstuff module\", role='article'):\n",
    "        for chapter in soup.find_all('div', class_=\"userstuff module\", role='article'):\n",
    "            output += chapter.get_text()+'\\n'\n",
    "    \n",
    "    elif soup.find('div', class_='userstuff'):\n",
    "        output += soup.find('div', class_='userstuff').get_text()\n",
    "    \n",
    "    else:\n",
    "        # print(f'can not find text for {id_}')\n",
    "        raise AssertionError\n",
    "        \n",
    "    return output, published_date\n",
    "\n",
    "def extract_info(id_, html):\n",
    "    stats = html.find('dl', class_='stats')\n",
    "    text, published_date = get_text_by_id(id_)\n",
    "    return {\n",
    "        'id': id_, # id\n",
    "        'title': html.find('h4', class_='heading').get_text().split('\\n')[1], # title\n",
    "        'author': html.find('h4', class_='heading').get_text().split('\\n')[-2],\n",
    "        'rating': html.find_all('span', class_=\"text\")[0].get_text(), # rating\n",
    "        'fandoms': [tag.get_text() for tag in html.find('h5', class_='fandoms heading').find_all('a')], # fandoms\n",
    "        'tags': [tag.get_text() for tag in html.find_all('li', class_='freeforms')], # tags\n",
    "        'warning': html.find_all('span', class_=\"text\")[1].get_text(), # warning\n",
    "        'pairing': html.find_all('span', class_=\"text\")[2].get_text(), # pairing,\n",
    "        'comments': stats.find('dd', class_='comments').get_text() if stats.find('dd', class_='comments') else 0,\n",
    "        'kudos': stats.find('dd', class_='kudos').get_text() if stats.find('dd', class_='kudos') else 0,\n",
    "        'hits': stats.find('dd', class_='hits').get_text(),\n",
    "        'relationships': [tag.get_text() for tag in html.find_all('li', class_='relationships')],\n",
    "        'characters': [tag.get_text() for tag in html.find_all('li', class_='characters')],\n",
    "        'summary': html.find('blockquote', class_='userstuff summary').get_text() if html.find('blockquote', class_='userstuff summary') else '',\n",
    "        'text': text, # content\n",
    "        'published_date': published_date,\n",
    "        'timestamp': datetime.datetime.now()\n",
    "    }\n",
    "    \n",
    "\n",
    "def is_valid(html):\n",
    "    status = html.find_all('span', class_=\"text\")[-1].get_text()\n",
    "    \n",
    "    stats = html.find('dl', class_='stats')\n",
    "    words_count = stats.find('dd', class_='words').get_text().replace(',','')\n",
    "    \n",
    "    if not words_count:\n",
    "        return False\n",
    "    \n",
    "    language = stats.find('dd', class_='language').get_text() \n",
    "    \n",
    "    return status == 'Complete Work' and language == 'English' \\\n",
    "        and int(words_count) > 1000 and int(words_count) < 25000\n",
    "\n",
    "    \n",
    "def get_ids_by_tag(tag, num_required, existed, page_offset=0):\n",
    "    url = 'https://archiveofourown.org/tags/{}/works?page={}'\n",
    "    headers = {'user-agent': 'bot (sj784@cornell.edu)'}\n",
    "    output = []\n",
    "    \n",
    "    page = 1 + page_offset\n",
    "    delay = 300\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        page_count = 0\n",
    "\n",
    "        r = requests.get(url.format(tag, page), headers=headers)\n",
    "        soup = BeautifulSoup(r.text)\n",
    "        \n",
    "        try:\n",
    "            error_cyle = 0\n",
    "            \n",
    "            works = soup.find_all('ol', class_='work index group')\n",
    "            assert len(works) > 0\n",
    "            \n",
    "            for fanfic in works[0].find_all('li', role='article'):\n",
    "                id_ = fanfic.find('h4', class_='heading').find('a', href=True)['href'].split('/')[-1]\n",
    "                \n",
    "                if id_ in existed:\n",
    "                    continue\n",
    "                if not is_valid(fanfic): \n",
    "                    continue\n",
    "                \n",
    "                output.append(extract_info(id_, fanfic))\n",
    "                page_count += 1\n",
    "\n",
    "            print(f'Scraped {page_count} from page {page}. Total: {len(output)}')\n",
    "            \n",
    "            if len(output) >= num_required:\n",
    "                break \n",
    "            \n",
    "            page += 1\n",
    "        \n",
    "        except AssertionError:\n",
    "            print(f'Timed out. Will try again in {delay} sec')\n",
    "            error_cyle += 1\n",
    "            \n",
    "            if error_cyle > 3:\n",
    "                print('Waited too long. Exit')\n",
    "                return output\n",
    "            \n",
    "            time.sleep(delay)\n",
    "            page += 5\n",
    "        \n",
    "\n",
    "    \n",
    "    print(f'Job complete. Scraped {len(output)} [{tag}] fanfics in total. Exit')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "brilliant-horror",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existed_df = pickle.load(open('data/ao3_db.p', 'rb'))\n",
    "old_ids = set(existed_df['id'].tolist())\n",
    "len(old_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-pixel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 5 from page 1006. Total: 5\n",
      "Scraped 7 from page 1007. Total: 12\n",
      "Scraped 6 from page 1008. Total: 18\n",
      "Scraped 6 from page 1009. Total: 24\n",
      "Scraped 6 from page 1010. Total: 30\n",
      "Scraped 11 from page 1011. Total: 41\n",
      "Scraped 2 from page 1012. Total: 43\n",
      "Scraped 2 from page 1013. Total: 45\n",
      "Scraped 6 from page 1014. Total: 51\n",
      "Scraped 3 from page 1015. Total: 54\n",
      "Scraped 7 from page 1016. Total: 61\n",
      "Scraped 5 from page 1017. Total: 66\n",
      "Scraped 4 from page 1018. Total: 70\n",
      "Scraped 5 from page 1019. Total: 75\n",
      "Scraped 1 from page 1020. Total: 76\n",
      "Scraped 8 from page 1021. Total: 84\n",
      "Scraped 6 from page 1022. Total: 90\n",
      "Scraped 6 from page 1023. Total: 96\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 5 from page 1029. Total: 106\n",
      "Job complete. Scraped 106 [Angst] fanfics in total. Exit\n",
      "Scraped 8 from page 1001. Total: 8\n",
      "Scraped 3 from page 1002. Total: 11\n",
      "Scraped 0 from page 1003. Total: 11\n",
      "Scraped 6 from page 1004. Total: 17\n",
      "Scraped 5 from page 1005. Total: 22\n",
      "Scraped 7 from page 1006. Total: 29\n",
      "Scraped 8 from page 1007. Total: 37\n",
      "Scraped 7 from page 1008. Total: 44\n",
      "Scraped 11 from page 1009. Total: 55\n",
      "Scraped 9 from page 1010. Total: 64\n",
      "Scraped 4 from page 1011. Total: 68\n",
      "Scraped 7 from page 1012. Total: 75\n",
      "Scraped 4 from page 1013. Total: 79\n",
      "Scraped 7 from page 1014. Total: 86\n",
      "Scraped 3 from page 1015. Total: 89\n",
      "Scraped 5 from page 1016. Total: 94\n",
      "Timed out. Will try again in 300 sec\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 6 from page 1027. Total: 103\n",
      "Job complete. Scraped 103 [Fluff] fanfics in total. Exit\n",
      "Scraped 9 from page 1001. Total: 9\n",
      "Scraped 0 from page 1002. Total: 9\n",
      "Scraped 7 from page 1003. Total: 16\n",
      "Scraped 11 from page 1004. Total: 27\n",
      "Scraped 13 from page 1005. Total: 40\n",
      "Scraped 14 from page 1006. Total: 54\n",
      "Scraped 8 from page 1007. Total: 62\n",
      "Scraped 11 from page 1008. Total: 73\n",
      "Scraped 8 from page 1009. Total: 81\n",
      "Scraped 7 from page 1010. Total: 88\n",
      "Scraped 14 from page 1011. Total: 102\n",
      "Job complete. Scraped 102 [Smut] fanfics in total. Exit\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 3 from page 1006. Total: 3\n",
      "Scraped 3 from page 1007. Total: 6\n",
      "Scraped 6 from page 1008. Total: 12\n",
      "Scraped 8 from page 1009. Total: 20\n",
      "Scraped 5 from page 1010. Total: 25\n",
      "Scraped 6 from page 1011. Total: 31\n",
      "Scraped 3 from page 1012. Total: 34\n",
      "Scraped 4 from page 1013. Total: 38\n",
      "Scraped 5 from page 1014. Total: 43\n",
      "Scraped 8 from page 1015. Total: 51\n",
      "Scraped 5 from page 1016. Total: 56\n",
      "Scraped 6 from page 1017. Total: 62\n",
      "Scraped 2 from page 1018. Total: 64\n",
      "Scraped 7 from page 1019. Total: 71\n",
      "Scraped 5 from page 1020. Total: 76\n",
      "Scraped 10 from page 1021. Total: 86\n",
      "Scraped 5 from page 1022. Total: 91\n",
      "Scraped 6 from page 1023. Total: 97\n",
      "Scraped 3 from page 1024. Total: 100\n",
      "Job complete. Scraped 100 [Romance] fanfics in total. Exit\n",
      "Timed out. Will try again in 300 sec\n"
     ]
    }
   ],
   "source": [
    "scraped_ids = set()\n",
    "data = []\n",
    "for iter_ in range(5):\n",
    "    print('Iteration', iter_+1)\n",
    "    for tag in ['Angst', 'Fluff', 'Smut', 'Romance', 'Alternate%20Canon', \n",
    "                'Alternate%20Universe', 'Relationship(s)', \n",
    "                'Hurt*s*Comfort', 'Sexual%20Content']:\n",
    "        old_ids |= scraped_ids\n",
    "        data += get_ids_by_tag(tag, 100, old_ids, 1000 + iter_ * 100)\n",
    "        scraped_ids = set([d['id'] for d in data])\n",
    "    print(f'Scraped {len(data)} so far')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ambient-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_scraped_to_df(df, scraped_raw):\n",
    "    scraped_df = pd.DataFrame(scraped_raw)\n",
    "    mask = scraped_df['text'].str.len() > 1000\n",
    "    scraped_df = scraped_df.loc[mask]\n",
    "    return df.append(scraped_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "public-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "existed_df = add_scraped_to_df(existed_df, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "american-collins",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(existed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "underlying-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(existed_df, open('data/ao3_db.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-valley",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
