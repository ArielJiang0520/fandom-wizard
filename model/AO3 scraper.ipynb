{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "incident-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm, trange\n",
    "import time\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "royal-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_by_id(id_):\n",
    "    url = 'https://archiveofourown.org/works/{}?view_full_work=true'\n",
    "    headers = {'user-agent': 'bot (sj784@cornell.edu)'}\n",
    "    \n",
    "    r = requests.get(url.format(id_), headers=headers)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    \n",
    "    assert soup != None\n",
    "        \n",
    "    stats = soup.find('dd', class_='stats')\n",
    "    \n",
    "    assert stats != None\n",
    "    \n",
    "    published_date = stats.find('dl', class_='stats').find('dd', class_='published').get_text() \\\n",
    "        if stats.find('dd', class_='published') else ''\n",
    "    \n",
    "    output = ''\n",
    "    if soup.find_all('div', class_=\"userstuff module\", role='article'):\n",
    "        for chapter in soup.find_all('div', class_=\"userstuff module\", role='article'):\n",
    "            output += chapter.get_text()+'\\n'\n",
    "    \n",
    "    elif soup.find('div', class_='userstuff'):\n",
    "        output += soup.find('div', class_='userstuff').get_text()\n",
    "    \n",
    "    else:\n",
    "        # print(f'can not find text for {id_}')\n",
    "        raise AssertionError\n",
    "        \n",
    "    return output, published_date\n",
    "\n",
    "def extract_info(id_, html):\n",
    "    stats = html.find('dl', class_='stats')\n",
    "    text, published_date = get_text_by_id(id_)\n",
    "    return {\n",
    "        'id': id_, # id\n",
    "        'title': html.find('h4', class_='heading').get_text().split('\\n')[1], # title\n",
    "        'author': html.find('h4', class_='heading').get_text().split('\\n')[-2],\n",
    "        'rating': html.find_all('span', class_=\"text\")[0].get_text(), # rating\n",
    "        'fandoms': [tag.get_text() for tag in html.find('h5', class_='fandoms heading').find_all('a')], # fandoms\n",
    "        'tags': [tag.get_text() for tag in html.find_all('li', class_='freeforms')], # tags\n",
    "        'warning': html.find_all('span', class_=\"text\")[1].get_text(), # warning\n",
    "        'pairing': html.find_all('span', class_=\"text\")[2].get_text(), # pairing,\n",
    "        'comments': stats.find('dd', class_='comments').get_text() if stats.find('dd', class_='comments') else 0,\n",
    "        'kudos': stats.find('dd', class_='kudos').get_text() if stats.find('dd', class_='kudos') else 0,\n",
    "        'hits': stats.find('dd', class_='hits').get_text(),\n",
    "        'relationships': [tag.get_text() for tag in html.find_all('li', class_='relationships')],\n",
    "        'characters': [tag.get_text() for tag in html.find_all('li', class_='characters')],\n",
    "        'summary': html.find('blockquote', class_='userstuff summary').get_text() if html.find('blockquote', class_='userstuff summary') else '',\n",
    "        'text': text, # content\n",
    "        'published_date': published_date,\n",
    "        'timestamp': datetime.datetime.now()\n",
    "    }\n",
    "    \n",
    "\n",
    "def is_valid(html):\n",
    "    status = html.find_all('span', class_=\"text\")[-1].get_text()\n",
    "    \n",
    "    stats = html.find('dl', class_='stats')\n",
    "    words_count = stats.find('dd', class_='words').get_text().replace(',','')\n",
    "    \n",
    "    if not words_count:\n",
    "        return False\n",
    "    \n",
    "    language = stats.find('dd', class_='language').get_text() \n",
    "    \n",
    "    return status == 'Complete Work' and language == 'English' \\\n",
    "        and int(words_count) > 1000 and int(words_count) < 25000\n",
    "\n",
    "    \n",
    "def get_ids_by_tag(tag, num_required, existed, page_offset=0):\n",
    "    url = 'https://archiveofourown.org/tags/{}/works?page={}'\n",
    "    headers = {'user-agent': 'bot (sj784@cornell.edu)'}\n",
    "    output = []\n",
    "    \n",
    "    page = 1 + page_offset\n",
    "    delay = 300\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        page_count = 0\n",
    "\n",
    "        r = requests.get(url.format(tag, page), headers=headers)\n",
    "        soup = BeautifulSoup(r.text)\n",
    "        \n",
    "        try:\n",
    "            error_cyle = 0\n",
    "            \n",
    "            works = soup.find_all('ol', class_='work index group')\n",
    "            assert len(works) > 0\n",
    "            \n",
    "            for fanfic in works[0].find_all('li', role='article'):\n",
    "                id_ = fanfic.find('h4', class_='heading').find('a', href=True)['href'].split('/')[-1]\n",
    "                \n",
    "                if id_ in existed:\n",
    "                    continue\n",
    "                if not is_valid(fanfic): \n",
    "                    continue\n",
    "                \n",
    "                output.append(extract_info(id_, fanfic))\n",
    "                page_count += 1\n",
    "\n",
    "            print(f'Scraped {page_count} from page {page}. Total: {len(output)}')\n",
    "            \n",
    "            if len(output) >= num_required:\n",
    "                break \n",
    "            \n",
    "            page += 1\n",
    "        \n",
    "        except AssertionError:\n",
    "            print(f'Timed out. Will try again in {delay} sec')\n",
    "            error_cyle += 1\n",
    "            \n",
    "            if error_cyle > 3:\n",
    "                print('Waited too long. Exit')\n",
    "                return output\n",
    "            \n",
    "            time.sleep(delay)\n",
    "            page += 5\n",
    "        \n",
    "\n",
    "    \n",
    "    print(f'Job complete. Scraped {len(output)} [{tag}] fanfics in total. Exit')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "understanding-police",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14056"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existed_df = pickle.load(open('data/ao3_db.p', 'rb'))\n",
    "old_ids = set(existed_df['id'].tolist())\n",
    "len(old_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-toronto",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Scraped 9 from page 3001. Total: 9\n",
      "Scraped 6 from page 3002. Total: 15\n",
      "Scraped 6 from page 3003. Total: 21\n",
      "Scraped 11 from page 3004. Total: 32\n",
      "Scraped 11 from page 3005. Total: 43\n",
      "Scraped 7 from page 3006. Total: 50\n",
      "Scraped 6 from page 3007. Total: 56\n",
      "Scraped 5 from page 3008. Total: 61\n",
      "Scraped 6 from page 3009. Total: 67\n",
      "Scraped 13 from page 3010. Total: 80\n",
      "Scraped 6 from page 3011. Total: 86\n",
      "Scraped 3 from page 3012. Total: 89\n",
      "Scraped 10 from page 3013. Total: 99\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 7 from page 3019. Total: 113\n",
      "Job complete. Scraped 113 [Angst] fanfics in total. Exit\n",
      "Scraped 8 from page 3001. Total: 8\n",
      "Scraped 5 from page 3002. Total: 13\n",
      "Scraped 5 from page 3003. Total: 18\n",
      "Scraped 7 from page 3004. Total: 25\n",
      "Scraped 5 from page 3005. Total: 30\n",
      "Scraped 3 from page 3006. Total: 33\n",
      "Scraped 5 from page 3007. Total: 38\n",
      "Scraped 6 from page 3008. Total: 44\n",
      "Scraped 4 from page 3009. Total: 48\n",
      "Scraped 12 from page 3010. Total: 60\n",
      "Scraped 6 from page 3011. Total: 66\n",
      "Scraped 8 from page 3012. Total: 74\n",
      "Scraped 9 from page 3013. Total: 83\n",
      "Scraped 9 from page 3014. Total: 92\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 10 from page 3020. Total: 107\n",
      "Job complete. Scraped 107 [Fluff] fanfics in total. Exit\n",
      "Scraped 9 from page 3001. Total: 9\n",
      "Scraped 7 from page 3002. Total: 16\n",
      "Scraped 10 from page 3003. Total: 26\n",
      "Scraped 14 from page 3004. Total: 40\n",
      "Scraped 8 from page 3005. Total: 48\n",
      "Scraped 11 from page 3006. Total: 59\n",
      "Scraped 9 from page 3007. Total: 68\n",
      "Scraped 10 from page 3008. Total: 78\n",
      "Scraped 10 from page 3009. Total: 88\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 9 from page 3015. Total: 108\n",
      "Job complete. Scraped 108 [Smut] fanfics in total. Exit\n",
      "Scraped 3 from page 3001. Total: 3\n",
      "Scraped 7 from page 3002. Total: 10\n",
      "Scraped 8 from page 3003. Total: 18\n",
      "Scraped 13 from page 3004. Total: 31\n",
      "Scraped 9 from page 3005. Total: 40\n",
      "Scraped 12 from page 3006. Total: 52\n",
      "Scraped 9 from page 3007. Total: 61\n",
      "Scraped 12 from page 3008. Total: 73\n",
      "Scraped 7 from page 3009. Total: 80\n",
      "Scraped 11 from page 3010. Total: 91\n",
      "Timed out. Will try again in 300 sec\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 13 from page 3021. Total: 112\n",
      "Job complete. Scraped 112 [Romance] fanfics in total. Exit\n",
      "Scraped 4 from page 3001. Total: 4\n",
      "Scraped 9 from page 3002. Total: 13\n",
      "Scraped 11 from page 3003. Total: 24\n",
      "Scraped 8 from page 3004. Total: 32\n",
      "Scraped 8 from page 3005. Total: 40\n",
      "Scraped 11 from page 3006. Total: 51\n",
      "Scraped 11 from page 3007. Total: 62\n",
      "Scraped 10 from page 3008. Total: 72\n",
      "Scraped 8 from page 3009. Total: 80\n",
      "Scraped 13 from page 3010. Total: 93\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 8 from page 3016. Total: 103\n",
      "Job complete. Scraped 103 [Alternate%20Canon] fanfics in total. Exit\n",
      "Scraped 3 from page 3001. Total: 3\n",
      "Scraped 8 from page 3002. Total: 11\n",
      "Scraped 8 from page 3003. Total: 19\n",
      "Scraped 8 from page 3004. Total: 27\n",
      "Scraped 7 from page 3005. Total: 34\n",
      "Scraped 9 from page 3006. Total: 43\n",
      "Scraped 6 from page 3007. Total: 49\n",
      "Scraped 8 from page 3008. Total: 57\n",
      "Scraped 5 from page 3009. Total: 62\n",
      "Scraped 4 from page 3010. Total: 66\n",
      "Scraped 11 from page 3011. Total: 77\n",
      "Scraped 9 from page 3012. Total: 86\n",
      "Scraped 7 from page 3013. Total: 93\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 4 from page 3019. Total: 101\n",
      "Job complete. Scraped 101 [Alternate%20Universe] fanfics in total. Exit\n",
      "Scraped 12 from page 3001. Total: 12\n",
      "Scraped 9 from page 3002. Total: 21\n",
      "Scraped 11 from page 3003. Total: 32\n",
      "Scraped 10 from page 3004. Total: 42\n",
      "Scraped 12 from page 3005. Total: 54\n",
      "Scraped 11 from page 3006. Total: 65\n",
      "Scraped 10 from page 3007. Total: 75\n",
      "Scraped 8 from page 3008. Total: 83\n",
      "Scraped 13 from page 3009. Total: 96\n",
      "Scraped 8 from page 3010. Total: 104\n",
      "Job complete. Scraped 104 [Relationship(s)] fanfics in total. Exit\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 8 from page 3006. Total: 8\n",
      "Scraped 13 from page 3007. Total: 21\n",
      "Scraped 11 from page 3008. Total: 32\n",
      "Scraped 8 from page 3009. Total: 40\n",
      "Scraped 11 from page 3010. Total: 51\n",
      "Scraped 10 from page 3011. Total: 61\n",
      "Scraped 9 from page 3012. Total: 70\n",
      "Scraped 8 from page 3013. Total: 78\n",
      "Scraped 11 from page 3014. Total: 89\n",
      "Scraped 14 from page 3015. Total: 103\n",
      "Job complete. Scraped 103 [Hurt*s*Comfort] fanfics in total. Exit\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 15 from page 3006. Total: 21\n",
      "Scraped 12 from page 3007. Total: 33\n",
      "Scraped 12 from page 3008. Total: 45\n",
      "Scraped 14 from page 3009. Total: 59\n",
      "Scraped 12 from page 3010. Total: 71\n",
      "Scraped 12 from page 3011. Total: 83\n",
      "Scraped 12 from page 3012. Total: 95\n",
      "Scraped 12 from page 3013. Total: 107\n",
      "Job complete. Scraped 107 [Sexual%20Content] fanfics in total. Exit\n",
      "Scraped 958 so far\n",
      "Iteration 2\n",
      "Scraped 6 from page 3101. Total: 6\n",
      "Timed out. Will try again in 300 sec\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 7 from page 3112. Total: 16\n",
      "Scraped 7 from page 3113. Total: 23\n",
      "Scraped 9 from page 3114. Total: 32\n",
      "Scraped 9 from page 3115. Total: 41\n",
      "Scraped 10 from page 3116. Total: 51\n",
      "Scraped 8 from page 3117. Total: 59\n",
      "Scraped 5 from page 3118. Total: 64\n",
      "Scraped 7 from page 3119. Total: 71\n",
      "Scraped 7 from page 3120. Total: 78\n",
      "Scraped 8 from page 3121. Total: 86\n",
      "Scraped 10 from page 3122. Total: 96\n",
      "Scraped 7 from page 3123. Total: 103\n",
      "Job complete. Scraped 103 [Angst] fanfics in total. Exit\n",
      "Scraped 6 from page 3101. Total: 6\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 7 from page 3107. Total: 19\n",
      "Scraped 6 from page 3108. Total: 25\n",
      "Scraped 9 from page 3109. Total: 34\n",
      "Scraped 8 from page 3110. Total: 42\n",
      "Scraped 7 from page 3111. Total: 49\n",
      "Scraped 5 from page 3112. Total: 54\n",
      "Scraped 9 from page 3113. Total: 63\n",
      "Scraped 10 from page 3114. Total: 73\n",
      "Scraped 8 from page 3115. Total: 81\n",
      "Scraped 7 from page 3116. Total: 88\n",
      "Scraped 13 from page 3117. Total: 101\n",
      "Job complete. Scraped 101 [Fluff] fanfics in total. Exit\n",
      "Scraped 10 from page 3101. Total: 10\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 15 from page 3107. Total: 33\n",
      "Scraped 12 from page 3108. Total: 45\n",
      "Scraped 9 from page 3109. Total: 54\n",
      "Scraped 16 from page 3110. Total: 70\n",
      "Scraped 11 from page 3111. Total: 81\n",
      "Scraped 8 from page 3112. Total: 89\n",
      "Scraped 11 from page 3113. Total: 100\n",
      "Job complete. Scraped 100 [Smut] fanfics in total. Exit\n",
      "Scraped 8 from page 3101. Total: 8\n",
      "Scraped 7 from page 3102. Total: 15\n",
      "Scraped 8 from page 3103. Total: 23\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 12 from page 3109. Total: 39\n",
      "Scraped 9 from page 3110. Total: 48\n",
      "Scraped 8 from page 3111. Total: 56\n",
      "Scraped 13 from page 3112. Total: 69\n",
      "Scraped 8 from page 3113. Total: 77\n",
      "Scraped 12 from page 3114. Total: 89\n",
      "Scraped 8 from page 3115. Total: 97\n",
      "Scraped 10 from page 3116. Total: 107\n",
      "Job complete. Scraped 107 [Romance] fanfics in total. Exit\n",
      "Scraped 10 from page 3101. Total: 10\n",
      "Scraped 11 from page 3102. Total: 21\n",
      "Scraped 5 from page 3103. Total: 26\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 10 from page 3109. Total: 38\n",
      "Scraped 9 from page 3110. Total: 47\n",
      "Scraped 12 from page 3111. Total: 59\n",
      "Scraped 8 from page 3112. Total: 67\n",
      "Scraped 9 from page 3113. Total: 76\n",
      "Scraped 9 from page 3114. Total: 85\n",
      "Scraped 8 from page 3115. Total: 93\n",
      "Scraped 8 from page 3116. Total: 101\n",
      "Job complete. Scraped 101 [Alternate%20Canon] fanfics in total. Exit\n",
      "Scraped 7 from page 3101. Total: 7\n",
      "Scraped 11 from page 3102. Total: 18\n",
      "Scraped 9 from page 3103. Total: 27\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 5 from page 3109. Total: 40\n",
      "Scraped 5 from page 3110. Total: 45\n",
      "Scraped 7 from page 3111. Total: 52\n",
      "Scraped 7 from page 3112. Total: 59\n",
      "Scraped 6 from page 3113. Total: 65\n",
      "Scraped 6 from page 3114. Total: 71\n",
      "Scraped 7 from page 3115. Total: 78\n",
      "Scraped 6 from page 3116. Total: 84\n",
      "Scraped 9 from page 3117. Total: 93\n",
      "Scraped 8 from page 3118. Total: 101\n",
      "Job complete. Scraped 101 [Alternate%20Universe] fanfics in total. Exit\n",
      "Scraped 7 from page 3101. Total: 7\n",
      "Scraped 11 from page 3102. Total: 18\n",
      "Scraped 13 from page 3103. Total: 31\n",
      "Timed out. Will try again in 300 sec\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 12 from page 3114. Total: 52\n",
      "Scraped 11 from page 3115. Total: 63\n",
      "Scraped 10 from page 3116. Total: 73\n",
      "Scraped 13 from page 3117. Total: 86\n",
      "Scraped 10 from page 3118. Total: 96\n",
      "Scraped 10 from page 3119. Total: 106\n",
      "Job complete. Scraped 106 [Relationship(s)] fanfics in total. Exit\n",
      "Scraped 12 from page 3101. Total: 12\n",
      "Scraped 10 from page 3102. Total: 22\n",
      "Scraped 9 from page 3103. Total: 31\n",
      "Scraped 10 from page 3104. Total: 41\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 10 from page 3110. Total: 53\n",
      "Scraped 10 from page 3111. Total: 63\n",
      "Scraped 12 from page 3112. Total: 75\n",
      "Scraped 10 from page 3113. Total: 85\n",
      "Scraped 12 from page 3114. Total: 97\n",
      "Scraped 11 from page 3115. Total: 108\n",
      "Job complete. Scraped 108 [Hurt*s*Comfort] fanfics in total. Exit\n",
      "Scraped 4 from page 3101. Total: 4\n",
      "Scraped 5 from page 3102. Total: 9\n",
      "Scraped 10 from page 3103. Total: 19\n",
      "Scraped 15 from page 3104. Total: 34\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 11 from page 3110. Total: 55\n",
      "Scraped 10 from page 3111. Total: 65\n",
      "Scraped 11 from page 3112. Total: 76\n",
      "Scraped 8 from page 3113. Total: 84\n",
      "Scraped 14 from page 3114. Total: 98\n",
      "Scraped 12 from page 3115. Total: 110\n",
      "Job complete. Scraped 110 [Sexual%20Content] fanfics in total. Exit\n",
      "Scraped 1895 so far\n",
      "Iteration 3\n",
      "Scraped 8 from page 3201. Total: 8\n",
      "Scraped 4 from page 3202. Total: 12\n",
      "Scraped 9 from page 3203. Total: 21\n",
      "Scraped 6 from page 3204. Total: 27\n",
      "Scraped 6 from page 3205. Total: 33\n",
      "Scraped 5 from page 3206. Total: 38\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 6 from page 3212. Total: 44\n",
      "Scraped 5 from page 3213. Total: 49\n",
      "Scraped 7 from page 3214. Total: 56\n",
      "Scraped 7 from page 3215. Total: 63\n",
      "Scraped 8 from page 3216. Total: 71\n",
      "Scraped 7 from page 3217. Total: 78\n",
      "Scraped 8 from page 3218. Total: 86\n",
      "Scraped 9 from page 3219. Total: 95\n",
      "Scraped 7 from page 3220. Total: 102\n",
      "Job complete. Scraped 102 [Angst] fanfics in total. Exit\n",
      "Scraped 12 from page 3201. Total: 12\n",
      "Scraped 8 from page 3202. Total: 20\n",
      "Scraped 8 from page 3203. Total: 28\n",
      "Scraped 3 from page 3204. Total: 31\n",
      "Scraped 9 from page 3205. Total: 40\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 11 from page 3211. Total: 52\n",
      "Scraped 11 from page 3212. Total: 63\n",
      "Scraped 9 from page 3213. Total: 72\n",
      "Scraped 14 from page 3214. Total: 86\n",
      "Scraped 9 from page 3215. Total: 95\n",
      "Scraped 7 from page 3216. Total: 102\n",
      "Job complete. Scraped 102 [Fluff] fanfics in total. Exit\n",
      "Scraped 9 from page 3201. Total: 9\n",
      "Scraped 12 from page 3202. Total: 21\n",
      "Scraped 13 from page 3203. Total: 34\n",
      "Scraped 10 from page 3204. Total: 44\n",
      "Timed out. Will try again in 300 sec\n",
      "Timed out. Will try again in 300 sec\n",
      "Scraped 11 from page 3215. Total: 59\n",
      "Scraped 14 from page 3216. Total: 73\n",
      "Scraped 10 from page 3217. Total: 83\n",
      "Scraped 13 from page 3218. Total: 96\n",
      "Scraped 8 from page 3219. Total: 104\n",
      "Job complete. Scraped 104 [Smut] fanfics in total. Exit\n",
      "Scraped 11 from page 3201. Total: 11\n",
      "Scraped 7 from page 3202. Total: 18\n",
      "Scraped 8 from page 3203. Total: 26\n",
      "Scraped 9 from page 3204. Total: 35\n",
      "Scraped 8 from page 3205. Total: 43\n",
      "Timed out. Will try again in 300 sec\n"
     ]
    }
   ],
   "source": [
    "scraped_ids = set()\n",
    "data = []\n",
    "for iter_ in range(10):\n",
    "    print('Iteration', iter_+1)\n",
    "    for tag in ['Angst', 'Fluff', 'Smut', 'Romance', 'Alternate%20Canon', \n",
    "                'Alternate%20Universe', 'Relationship(s)', \n",
    "                'Hurt*s*Comfort', 'Sexual%20Content']:\n",
    "        old_ids |= scraped_ids\n",
    "        data += get_ids_by_tag(tag, 100, old_ids, 3000 + iter_ * 100)\n",
    "        scraped_ids = set([d['id'] for d in data])\n",
    "    print(f'Scraped {len(data)} so far')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "swiss-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_scraped_to_df(df, scraped_raw):\n",
    "    scraped_df = pd.DataFrame(scraped_raw)\n",
    "    mask = scraped_df['text'].str.len() > 1000\n",
    "    scraped_df = scraped_df.loc[mask]\n",
    "    return df.append(scraped_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "iraqi-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "existed_df = add_scraped_to_df(existed_df, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "identified-mobile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14187"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(existed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "leading-vermont",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>fandoms</th>\n",
       "      <th>tags</th>\n",
       "      <th>warning</th>\n",
       "      <th>pairing</th>\n",
       "      <th>comments</th>\n",
       "      <th>kudos</th>\n",
       "      <th>hits</th>\n",
       "      <th>relationships</th>\n",
       "      <th>characters</th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "      <th>published_date</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31166018</td>\n",
       "      <td>Don't Worry, You Are More Than Enough;</td>\n",
       "      <td>Bang_Daddy_Chan</td>\n",
       "      <td>Not Rated</td>\n",
       "      <td>[Stray Kids (Band)]</td>\n",
       "      <td>[Hwang Hyunjin is a Mess, Hwang Hyunjin is a P...</td>\n",
       "      <td>Choose Not To Use Archive Warnings, No Archive...</td>\n",
       "      <td>M/M, Multi, Other</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>502</td>\n",
       "      <td>[Hwang Hyunjin/Yang Jeongin | I.N, Bang Chan/H...</td>\n",
       "      <td>[Hwang Hyunjin, Yang Jeongin | I.N, Lee Minho ...</td>\n",
       "      <td>\\nThis spot where Hyunjin was now, used to be ...</td>\n",
       "      <td>Hyunjin couldn't focus. It wasn't something th...</td>\n",
       "      <td>2021-05-08</td>\n",
       "      <td>2021-05-15 18:01:46.777424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28520001</td>\n",
       "      <td>About That Kind of Desire...</td>\n",
       "      <td>Mina_chan95</td>\n",
       "      <td>Mature</td>\n",
       "      <td>[King of Fighters]</td>\n",
       "      <td>[you know I had to do it to 'em, Kyo didn't le...</td>\n",
       "      <td>Graphic Depictions Of Violence</td>\n",
       "      <td>Gen</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>47</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Kusanagi Kyou, Kusanagi (King of Fighters), K...</td>\n",
       "      <td>\\n[Sequel to Perishing Little Flame on Winding...</td>\n",
       "      <td>\\nChapter Text\\nCouple months have passed sinc...</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>2021-05-15 18:01:46.995844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30910631</td>\n",
       "      <td>❃ 𝐄𝐏𝐇𝐄𝐌𝐄𝐑𝐀𝐋 || ᴘɪᴇᴛʀᴏ ᴍᴀxɪᴍᴏꜰꜰ ❃</td>\n",
       "      <td>MiniSized</td>\n",
       "      <td>Teen And Up Audiences</td>\n",
       "      <td>[Marvel Cinematic Universe, Marvel]</td>\n",
       "      <td>[Avengers: Age of Ultron (Movie), Fluff, Angst...</td>\n",
       "      <td>Graphic Depictions Of Violence, Major Characte...</td>\n",
       "      <td>No category</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>416</td>\n",
       "      <td>[Pietro Maximoff/Reader]</td>\n",
       "      <td>[Wanda Maximoff, Pietro Maximoff, Vision (Marv...</td>\n",
       "      <td>\\n⁽ᵃᵈʲ‧⁾ ˡᵃˢᵗⁱⁿᵍ ᶠᵒʳ ᵃ ᵛᵉʳʸ ˢʰᵒʳᵗ ᵃᵐᵒᵘⁿᵗ ᵒᶠ ᵗⁱ...</td>\n",
       "      <td>\\nChapter Text\\n  Sokovia, Europe  HYDRA Resea...</td>\n",
       "      <td>2021-04-26</td>\n",
       "      <td>2021-05-15 18:01:47.270095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31165844</td>\n",
       "      <td>Warmth part II</td>\n",
       "      <td>for JoeyWrites, jordypordy</td>\n",
       "      <td>Not Rated</td>\n",
       "      <td>[NieR: Automata (Video Game)]</td>\n",
       "      <td>[Angst, Hurt/Comfort, Fluff, Homoeroticism, En...</td>\n",
       "      <td>Choose Not To Use Archive Warnings</td>\n",
       "      <td>M/M</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>[9S/801S (NieR: Automata)]</td>\n",
       "      <td>[9S (NieR: Automata), 801S (NieR: Automata)]</td>\n",
       "      <td>\\ntwo star-crossed lovers have one final chanc...</td>\n",
       "      <td>\\nHe never comes back. This much 801S knows. E...</td>\n",
       "      <td>2021-05-08</td>\n",
       "      <td>2021-05-15 18:01:48.614115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31165823</td>\n",
       "      <td>love in the dark</td>\n",
       "      <td>ellyxts</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>[Haikyuu!!]</td>\n",
       "      <td>[Angst, literally just angst, No Fluff, sorry ...</td>\n",
       "      <td>No Archive Warnings Apply</td>\n",
       "      <td>M/M</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>331</td>\n",
       "      <td>[Miya Atsumu/Sakusa Kiyoomi]</td>\n",
       "      <td>[Sakusa Kiyoomi, Miya Atsumu]</td>\n",
       "      <td>\\nthe argument grew from nowhere into a sudden...</td>\n",
       "      <td>\"the person ya were, the one i fell in love wi...</td>\n",
       "      <td>2021-05-08</td>\n",
       "      <td>2021-05-15 18:01:48.796242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14182</th>\n",
       "      <td>30246960</td>\n",
       "      <td>\"Better Than the Credits\" Shirou x reader (sli...</td>\n",
       "      <td>Genderfluid_insomniac</td>\n",
       "      <td>Mature</td>\n",
       "      <td>[BNA: Brand New Animal (Anime)]</td>\n",
       "      <td>[Fluff, Smut, Fluff and Smut, theyre both over...</td>\n",
       "      <td>Choose Not To Use Archive Warnings</td>\n",
       "      <td>F/M, Gen, Multi</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>125</td>\n",
       "      <td>[Ogami Shirou/Reader]</td>\n",
       "      <td>[Kagemori Michiru, Ogami Shirou]</td>\n",
       "      <td>\\nShirou and Y/N have the house alone as are w...</td>\n",
       "      <td>Shirou and Y/N were sitting around on a Friday...</td>\n",
       "      <td>2021-03-24</td>\n",
       "      <td>2021-05-16 12:22:03.753465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14183</th>\n",
       "      <td>30246939</td>\n",
       "      <td>all up in the air</td>\n",
       "      <td>for yanak324</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>[A Song of Ice and Fire - George R. R. Martin]</td>\n",
       "      <td>[fuck i forgot how to tag fics, Alternate Univ...</td>\n",
       "      <td>Choose Not To Use Archive Warnings</td>\n",
       "      <td>F/M</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>125</td>\n",
       "      <td>[Lyanna Mormont/Rickon Stark]</td>\n",
       "      <td>[Rickon Stark, Lyanna Mormont]</td>\n",
       "      <td>\\nRickon and Lyanna cross paths at their high ...</td>\n",
       "      <td>\\n\\n\\n\\nEven amongst the chatter of familiar s...</td>\n",
       "      <td>2021-03-24</td>\n",
       "      <td>2021-05-16 12:22:03.957424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14184</th>\n",
       "      <td>30246948</td>\n",
       "      <td>cosmical laughter</td>\n",
       "      <td>VisionaryPowerhouse</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>[Voltron: Legendary Defender]</td>\n",
       "      <td>[Astral Plane Fucking, hell yeah, relationship...</td>\n",
       "      <td>Choose Not To Use Archive Warnings</td>\n",
       "      <td>M/M</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>396</td>\n",
       "      <td>[Keith/Shiro (Voltron)]</td>\n",
       "      <td>[Keith (Voltron), Shiro (Voltron), Black Lion ...</td>\n",
       "      <td>\\nKeith has a little surprise encounter in the...</td>\n",
       "      <td>As if Keith wasn’t already thinking about how ...</td>\n",
       "      <td>2021-03-24</td>\n",
       "      <td>2021-05-16 12:22:04.115457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14185</th>\n",
       "      <td>30246921</td>\n",
       "      <td>Imagine This....</td>\n",
       "      <td>MrsParkJimin18</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>[No Fandom]</td>\n",
       "      <td>[Sexual Fantasy, Lesbian Sex, Oral Sex, Vagina...</td>\n",
       "      <td>Choose Not To Use Archive Warnings</td>\n",
       "      <td>F/F</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>1057</td>\n",
       "      <td>[Reader/Undisclosed]</td>\n",
       "      <td>[Reader]</td>\n",
       "      <td>\\nJust a little scenario to use your imaginati...</td>\n",
       "      <td>\\nIt’s a quiet Saturday night, the only plans ...</td>\n",
       "      <td>2021-03-25</td>\n",
       "      <td>2021-05-16 12:22:04.280040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14186</th>\n",
       "      <td>30246918</td>\n",
       "      <td>First Meetings</td>\n",
       "      <td>FluffMarshmallow (ResidentEvilNerd98)</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>[Final Fantasy XV]</td>\n",
       "      <td>[First Meeting, Meet-Cute, Oral Sex, Vaginal F...</td>\n",
       "      <td>Choose Not To Use Archive Warnings</td>\n",
       "      <td>F/M</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>[Original Female Character/Original Male Chara...</td>\n",
       "      <td>[Erebus Ulric, Venere Le'Claire]</td>\n",
       "      <td>\\nThe young woman looked up at him properly, g...</td>\n",
       "      <td>It was late, stars already twinkling brightly ...</td>\n",
       "      <td>2021-03-24</td>\n",
       "      <td>2021-05-16 12:22:05.157377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14187 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                              title  \\\n",
       "0      31166018             Don't Worry, You Are More Than Enough;   \n",
       "1      28520001                       About That Kind of Desire...   \n",
       "2      30910631                   ❃ 𝐄𝐏𝐇𝐄𝐌𝐄𝐑𝐀𝐋 || ᴘɪᴇᴛʀᴏ ᴍᴀxɪᴍᴏꜰꜰ ❃   \n",
       "3      31165844                                     Warmth part II   \n",
       "4      31165823                                   love in the dark   \n",
       "...         ...                                                ...   \n",
       "14182  30246960  \"Better Than the Credits\" Shirou x reader (sli...   \n",
       "14183  30246939                                  all up in the air   \n",
       "14184  30246948                                  cosmical laughter   \n",
       "14185  30246921                                   Imagine This....   \n",
       "14186  30246918                                     First Meetings   \n",
       "\n",
       "                                      author                 rating  \\\n",
       "0                            Bang_Daddy_Chan              Not Rated   \n",
       "1                                Mina_chan95                 Mature   \n",
       "2                                  MiniSized  Teen And Up Audiences   \n",
       "3                 for JoeyWrites, jordypordy              Not Rated   \n",
       "4                                    ellyxts               Explicit   \n",
       "...                                      ...                    ...   \n",
       "14182                  Genderfluid_insomniac                 Mature   \n",
       "14183                           for yanak324               Explicit   \n",
       "14184                    VisionaryPowerhouse               Explicit   \n",
       "14185                         MrsParkJimin18               Explicit   \n",
       "14186  FluffMarshmallow (ResidentEvilNerd98)               Explicit   \n",
       "\n",
       "                                              fandoms  \\\n",
       "0                                 [Stray Kids (Band)]   \n",
       "1                                  [King of Fighters]   \n",
       "2                 [Marvel Cinematic Universe, Marvel]   \n",
       "3                       [NieR: Automata (Video Game)]   \n",
       "4                                         [Haikyuu!!]   \n",
       "...                                               ...   \n",
       "14182                 [BNA: Brand New Animal (Anime)]   \n",
       "14183  [A Song of Ice and Fire - George R. R. Martin]   \n",
       "14184                   [Voltron: Legendary Defender]   \n",
       "14185                                     [No Fandom]   \n",
       "14186                              [Final Fantasy XV]   \n",
       "\n",
       "                                                    tags  \\\n",
       "0      [Hwang Hyunjin is a Mess, Hwang Hyunjin is a P...   \n",
       "1      [you know I had to do it to 'em, Kyo didn't le...   \n",
       "2      [Avengers: Age of Ultron (Movie), Fluff, Angst...   \n",
       "3      [Angst, Hurt/Comfort, Fluff, Homoeroticism, En...   \n",
       "4      [Angst, literally just angst, No Fluff, sorry ...   \n",
       "...                                                  ...   \n",
       "14182  [Fluff, Smut, Fluff and Smut, theyre both over...   \n",
       "14183  [fuck i forgot how to tag fics, Alternate Univ...   \n",
       "14184  [Astral Plane Fucking, hell yeah, relationship...   \n",
       "14185  [Sexual Fantasy, Lesbian Sex, Oral Sex, Vagina...   \n",
       "14186  [First Meeting, Meet-Cute, Oral Sex, Vaginal F...   \n",
       "\n",
       "                                                 warning            pairing  \\\n",
       "0      Choose Not To Use Archive Warnings, No Archive...  M/M, Multi, Other   \n",
       "1                         Graphic Depictions Of Violence                Gen   \n",
       "2      Graphic Depictions Of Violence, Major Characte...        No category   \n",
       "3                     Choose Not To Use Archive Warnings                M/M   \n",
       "4                              No Archive Warnings Apply                M/M   \n",
       "...                                                  ...                ...   \n",
       "14182                 Choose Not To Use Archive Warnings    F/M, Gen, Multi   \n",
       "14183                 Choose Not To Use Archive Warnings                F/M   \n",
       "14184                 Choose Not To Use Archive Warnings                M/M   \n",
       "14185                 Choose Not To Use Archive Warnings                F/F   \n",
       "14186                 Choose Not To Use Archive Warnings                F/M   \n",
       "\n",
       "      comments kudos  hits                                      relationships  \\\n",
       "0            4    52   502  [Hwang Hyunjin/Yang Jeongin | I.N, Bang Chan/H...   \n",
       "1           14     5    47                                                 []   \n",
       "2            0    18   416                           [Pietro Maximoff/Reader]   \n",
       "3            6     6    48                         [9S/801S (NieR: Automata)]   \n",
       "4            8    16   331                       [Miya Atsumu/Sakusa Kiyoomi]   \n",
       "...        ...   ...   ...                                                ...   \n",
       "14182        0     2   125                              [Ogami Shirou/Reader]   \n",
       "14183        2     8   125                      [Lyanna Mormont/Rickon Stark]   \n",
       "14184        0    18   396                            [Keith/Shiro (Voltron)]   \n",
       "14185        6    27  1057                               [Reader/Undisclosed]   \n",
       "14186        0     2    35  [Original Female Character/Original Male Chara...   \n",
       "\n",
       "                                              characters  \\\n",
       "0      [Hwang Hyunjin, Yang Jeongin | I.N, Lee Minho ...   \n",
       "1      [Kusanagi Kyou, Kusanagi (King of Fighters), K...   \n",
       "2      [Wanda Maximoff, Pietro Maximoff, Vision (Marv...   \n",
       "3           [9S (NieR: Automata), 801S (NieR: Automata)]   \n",
       "4                          [Sakusa Kiyoomi, Miya Atsumu]   \n",
       "...                                                  ...   \n",
       "14182                   [Kagemori Michiru, Ogami Shirou]   \n",
       "14183                     [Rickon Stark, Lyanna Mormont]   \n",
       "14184  [Keith (Voltron), Shiro (Voltron), Black Lion ...   \n",
       "14185                                           [Reader]   \n",
       "14186                   [Erebus Ulric, Venere Le'Claire]   \n",
       "\n",
       "                                                 summary  \\\n",
       "0      \\nThis spot where Hyunjin was now, used to be ...   \n",
       "1      \\n[Sequel to Perishing Little Flame on Winding...   \n",
       "2      \\n⁽ᵃᵈʲ‧⁾ ˡᵃˢᵗⁱⁿᵍ ᶠᵒʳ ᵃ ᵛᵉʳʸ ˢʰᵒʳᵗ ᵃᵐᵒᵘⁿᵗ ᵒᶠ ᵗⁱ...   \n",
       "3      \\ntwo star-crossed lovers have one final chanc...   \n",
       "4      \\nthe argument grew from nowhere into a sudden...   \n",
       "...                                                  ...   \n",
       "14182  \\nShirou and Y/N have the house alone as are w...   \n",
       "14183  \\nRickon and Lyanna cross paths at their high ...   \n",
       "14184  \\nKeith has a little surprise encounter in the...   \n",
       "14185  \\nJust a little scenario to use your imaginati...   \n",
       "14186  \\nThe young woman looked up at him properly, g...   \n",
       "\n",
       "                                                    text published_date  \\\n",
       "0      Hyunjin couldn't focus. It wasn't something th...     2021-05-08   \n",
       "1      \\nChapter Text\\nCouple months have passed sinc...     2021-01-04   \n",
       "2      \\nChapter Text\\n  Sokovia, Europe  HYDRA Resea...     2021-04-26   \n",
       "3      \\nHe never comes back. This much 801S knows. E...     2021-05-08   \n",
       "4      \"the person ya were, the one i fell in love wi...     2021-05-08   \n",
       "...                                                  ...            ...   \n",
       "14182  Shirou and Y/N were sitting around on a Friday...     2021-03-24   \n",
       "14183  \\n\\n\\n\\nEven amongst the chatter of familiar s...     2021-03-24   \n",
       "14184  As if Keith wasn’t already thinking about how ...     2021-03-24   \n",
       "14185  \\nIt’s a quiet Saturday night, the only plans ...     2021-03-25   \n",
       "14186  It was late, stars already twinkling brightly ...     2021-03-24   \n",
       "\n",
       "                       timestamp  \n",
       "0     2021-05-15 18:01:46.777424  \n",
       "1     2021-05-15 18:01:46.995844  \n",
       "2     2021-05-15 18:01:47.270095  \n",
       "3     2021-05-15 18:01:48.614115  \n",
       "4     2021-05-15 18:01:48.796242  \n",
       "...                          ...  \n",
       "14182 2021-05-16 12:22:03.753465  \n",
       "14183 2021-05-16 12:22:03.957424  \n",
       "14184 2021-05-16 12:22:04.115457  \n",
       "14185 2021-05-16 12:22:04.280040  \n",
       "14186 2021-05-16 12:22:05.157377  \n",
       "\n",
       "[14187 rows x 17 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "provincial-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(existed_df, open('data/ao3_db.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-grant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
